# Watermarks in Stream Processing

Watermarks are a crucial mechanism in stream processing systems for handling out-of-order data and reasoning about the completeness of data over time, especially in unbounded streams.

## What is a Watermark?

A **watermark** represents the temporal completeness of an out-of-order data stream. Its current value signals to a processor that all messages with an event time timestamp lower than the watermark's timestamp have been received. In simpler terms, if a watermark with a timestamp *t* is observed, the system understands that it will not see any more records with an event time earlier than or equal to *t*.

Watermarks establish a correspondence between **processing time** (when events are observed or processed) and **event time** (when events actually occurred in the real world). They must be **monotonic**, meaning they should never move backward, and ideally **conformant**, meaning they accurately reflect the event times. However, in practice, heuristic or non-conformant watermarks are also used.

## Why are Watermarks Needed in Stream Processing?

The unbounded and temporally disordered nature of real-world data streams presents a significant challenge: determining the completeness of a stream that never ends. Watermarks are essential for addressing this challenge and enable several key functionalities:

* **Computing a Single Correct Answer:** For use cases like notifications or alerts, where a single, definitive result is required rather than a stream of incrementally refined partial results. Watermarks provide a completeness signal to generate this conclusive result based on inputs from a specific time range.
* **Reasoning About a Lack of Data:** In scenarios like dip detection (e.g., identifying when website visits drop below a threshold), watermarks help distinguish between a genuine dip in event rate and lagging input data. They allow the system to delay evaluation until all necessary inputs for the period are likely observed.
* **Performing Non-Incremental Processing:** Some computations are inherently non-incremental, such as complex statistical models that require full datasets for a given time window. Watermarks allow delaying such computations until all relevant inputs have arrived, avoiding costly recomputations.
* **Safe and Punctual Garbage Collection:** Watermarks are vital for managing state in stateful computations over infinite streams. They help determine when it's safe to discard obsolete input data and intermediate state, preventing infinite storage growth.
* **Surfacing Pipeline Health:** A well-instrumented watermark system offers a reliable signal of the overall health and progress of a data processing pipeline. Delays in watermarks can often help pinpoint issues within the pipeline.

## How Watermarks Work in Apache Flink

Apache Flink implements a watermark mechanism that supports event-time processing semantics, where watermarks are generally computed and propagated through the pipeline along with the data itself.

### 1. Watermark Generation (At the Source)

In Flink, watermarks can be generated either directly at the source nodes or in dedicated watermark generation nodes.

* **Source-Generated Watermarks:** Source nodes can compute watermarks based on the characteristics of the ingested elements or leverage metadata provided by the external system they are reading from (e.g., partitions, offsets, or timestamps in Kafka). For instance, if a Kafka source knows its partitions contain monotonically increasing timestamps, it can generate a conformant watermark based on the minimum of the latest timestamps seen across all partitions.
* **Dedicated Watermark Generators:** These nodes compute watermarks based solely on the timestamps of the input elements they have observed.
* **Publishing Mechanisms:** Flink offers two ways to publish watermarks:
    * Periodically: Nodes can maintain a watermark and publish it at a configurable interval.
    * Per-Record: Nodes can directly publish watermarks, potentially for every input record processed.
* **Monotonicity:** Flink ensures the strict monotonicity of watermarks by only forwarding watermarks that are larger than the previously emitted one.
* **Types of Watermarks:**
    * **Conformant ("Perfect") Watermarks:** These accurately reflect that no earlier events will arrive. Generating these is challenging and often depends on specific characteristics of the data source.
    * **Non-Conformant ("Heuristic") Watermarks:** Used when guaranteeing conformance is impractical. These rely on heuristics like bounded disorder assumptions (advancing the watermark to `event_time - Î”`) or timeouts. More sophisticated heuristics might involve statistical modeling of event source behavior. Late data (elements arriving after the watermark has passed their timestamp) can occur with non-conformant watermarks.

### 2. Watermark Propagation and Consumption (At Operators)

Once generated, watermarks need to be propagated through the dataflow graph to downstream operators.

* **Propagation:**
    * Flink nodes propagate watermarks by emitting them as special metadata messages alongside regular data records within the data streams.
    * Since Flink's communication channels preserve message order, a receiving operator ingests watermarks and data messages in the same sequence they were sent.
    * Each operator node keeps track of the maximum watermark received on each of its input edges. The operator's current watermark is typically the minimum of these input watermarks. When a new watermark message arrives, the operator updates the corresponding input edge's watermark and recalculates its own watermark.
* **Consumption (Reacting to Advancing Watermarks):**
    * Operators use watermarks to understand the progress of event time. The primary way they react to advancing watermarks is through **watermark timers**.
    * Programmers can schedule callbacks (timers) to be executed when an operator's watermark reaches or exceeds a certain timestamp.
    * This is commonly used in windowing operations. For example, when processing hourly windows, a timer can be set for the end of each hour. When the watermark passes the end-of-window timestamp, the timer triggers, and the operator can finalize and emit the result for that window.
    * When an operator's watermark advances, it processes all pending timers whose timestamps are less than the new watermark value, invoking their respective callbacks. These callbacks can then send messages downstream or modify the operator's state.
* **Handling Multiple Inputs:** When an operator has multiple input streams, its event time clock is typically determined by the minimum of the watermarks from all its input streams. This ensures that the operator only processes data up to the point where all its inputs are considered complete.
* **Stateful Operators:** Stateful operators (like those performing aggregations, joins, or grouping) use watermarks to know when it's safe to finalize computations for a given time window and potentially clear out state associated with that window. For example, an aggregation operator might buffer incoming messages or partial aggregates in its state until the watermark indicates that all messages for a particular aggregate (e.g., a time window) have arrived.

### 3. Watermarks at Sinks

While the provided document focuses extensively on watermark generation at sources and propagation/consumption at operators, the behavior at sinks is a natural extension:

* **Determining Finality:** Sinks, like any other operator, receive watermarks from their upstream operators. A sink can use the incoming watermark to understand when data for a particular event time window is considered complete by the upstream processing logic.
* **Triggering Actions:** This "completeness" signal can then be used by the sink to trigger actions like:
    * Writing final aggregated results to an external database or file system.
    * Sending notifications or alerts.
    * Committing transactions.
* In essence, the watermark tells the sink that it is unlikely (or impossible, in the case of perfect watermarks) to receive further data for event times preceding the watermark. This allows the sink to perform its final operations with a degree of certainty about the completeness of the data it has processed for that time period.

### Challenges and Considerations in Flink:

* **Fault Recovery:** In Flink, if a node fails, its watermark is reset, and the pipeline typically needs to rewind to the last checkpoint. Watermarks must then be propagated again from the sources.
* **Idle Sources:** If a source task has no new input data, it can stall watermark progress for the entire pipeline. Flink allows such sources to declare themselves "idle," temporarily excluding them from watermark calculations by downstream operators.
* **Bottlenecks and Skew:** A single slow or bottlenecked operator can obstruct watermark progress for all downstream operators. Similarly, if different sources have watermarks progressing at vastly different rates (watermark skew), it can lead to increased state size for buffering. Flink provides mechanisms to help manage skew.

In summary, watermarks in Flink provide a robust system for managing event time and out-of-order data, enabling correct and efficient stream processing across sources, operators, and ultimately influencing actions at sinks.



When using Apache Kafka as a source in an Apache Flink streaming application, watermarks are crucial for enabling event-time processing. Here's how Flink typically handles watermark generation with Kafka sources:

**Core Principle: Per-Partition Watermarks**

The most common strategy relies on the characteristics of Kafka partitions:

1.  **Timestamp Assignment:** For event-time processing to work, messages in Kafka topics must have timestamps. These can be:
    * **Event Timestamps:** Timestamps embedded within the message itself, indicating when the event actually occurred. Flink will need to be configured to extract these.
    * **LogAppendTime (Kafka Ingestion Time):** If event timestamps are not available or reliable, Kafka brokers can assign a timestamp when the message is appended to the log. This is less accurate for true event time but can still be used.

2.  **Assumption of Order (Within a Partition):** A key assumption often made is that messages *within a single Kafka partition* are ordered by their timestamps, or at least that timestamps are monotonically non-decreasing. Kafka guarantees message order within a partition, but this doesn't automatically mean event timestamps are perfectly ordered if multiple producers write to that partition or if events themselves were generated out of order before being produced.
    * If timestamps are indeed monotonically increasing per partition, Flink can generate a fairly accurate (potentially conformant) watermark. [cite: 402]

3.  **Tracking Maximum Timestamps:** The Flink Kafka source connector, when configured for watermark generation, will typically track the maximum event timestamp it has observed *for each Kafka partition it is reading from*.

4.  **Calculating the Overall Watermark:** The watermark for the entire Kafka source (across all its parallel reader instances, each handling one or more partitions) is then determined by the **minimum** of these maximum observed timestamps across all assigned partitions. [cite: 380]
    * For example, if a Flink source task is reading from three Kafka partitions and the latest timestamps seen are:
        * Partition 1: 10:00:05
        * Partition 2: 10:00:03
        * Partition 3: 10:00:06
    * The current watermark for that source task would be based on 10:00:03 (the minimum), because it cannot yet guarantee that no older events will arrive from Partition 2.

5.  **Built-in Watermark Strategies:** Flink provides several built-in `WatermarkStrategy` implementations that can be used with Kafka sources, or you can implement a custom one:
    * **Bounded Out-of-Orderness:** This is a common strategy. You specify a maximum delay (e.g., 5 seconds) by which events are expected to be out of order. The watermark will then be `max_event_time_seen_so_far - max_delay`.
    * **Monotonously Increasing Timestamps:** If you can guarantee that timestamps are strictly ascending per partition, this strategy can be used. The watermark is typically the timestamp of the last processed record minus a small delta (e.g., 1 millisecond) to handle potential duplicate timestamps.
    * **Custom Watermark Generators:** For more complex scenarios, you can define your own logic for generating watermarks based on the incoming Kafka records.

**How it Works in Practice:**

* Flink source nodes (specifically, the instances of the Kafka consumer) can compute watermarks based on the ingested elements or leverage metadata provided by Kafka, such as system-managed partitions and the timestamps within the records. [cite: 266]
* For a Kafka topic where partitions are known to contain monotonically increasing timestamps, a conformant watermark generator can be implemented. [cite: 402] This means the watermark accurately reflects that no earlier events will arrive.
* When Flink reads from an ordered-by-partition source like Kafka, it can assume the ordering of timestamps per partition and use the minimum of the latest timestamp seen across those partitions to determine the current watermark. [cite: 380]

**Important Considerations:**

* **Timestamp Extraction:** You must correctly configure Flink to extract the event timestamp from the Kafka `ConsumerRecord`.
* **Handling Idle Partitions:** If a Kafka partition becomes idle (no new messages), its last seen timestamp might hold back the overall watermark for the source. Flink's watermark strategies often have mechanisms to deal with idle sources to prevent the entire pipeline from stalling.
* **Out-of-Orderness:** The degree of out-of-orderness in your Kafka messages significantly impacts the choice of watermark strategy and the potential latency introduced. If events can be significantly delayed or arrive very out of order, you'll need a larger bounded-out-of-orderness delay, which can increase processing latency as Flink waits longer for potentially late events.
* **Kafka Topic Configuration:** The number of partitions in your Kafka topic can affect parallelism and how watermarks are aggregated.

By generating watermarks at the source based on the event timestamps in Kafka messages, Flink can effectively process data based on when events actually occurred, even if those events arrive out of order or are delayed.
